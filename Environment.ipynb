{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KS6wwFRc9CQK"
   },
   "source": [
    "# Building the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cslVfkBSLean"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import Networks\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.envs.registration import EnvSpec\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.dqn.dqn_policy_graph import *\n",
    "from ray.rllib.agents.ppo.ppo_policy_graph import *\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.models.preprocessors import DictFlatteningPreprocessor, Preprocessor\n",
    "\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8YRoyVY687z"
   },
   "source": [
    "### Environment Below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73fz3Ogq6870"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-18aee50a21c6>, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-18aee50a21c6>\"\u001b[0;36m, line \u001b[0;32m84\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gamma = 0\n",
    "\n",
    "def reward_calculator(travel_time, marginal_cost, soc_fac):\n",
    "    # ADD THE BREMIAN DIVERGENCE!!!\n",
    "    rew_dict = {}\n",
    "    total_travel_time = 0\n",
    "    for p, tt in travel_time.items():\n",
    "        total_travel_time += tt\n",
    "    for agent in travel_time.keys():\n",
    "        # rew_dict[agent] = - (travel_time[agent] + soc_fac * marginal_cost[agent])\n",
    "        rew_dict[agent] = - total_travel_time\n",
    "    return rew_dict\n",
    "\n",
    "## Routing Environment \n",
    "class RoutingEnv(MultiAgentEnv):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        The cars start at the same origin point, Point A, and need to reach the same destination, Point Z. \n",
    "        Each car can reach Point Z via a variety of routing choices described in a given network, in which \n",
    "        each route introduces different travel times and congestion. \n",
    "        The goal is to minimize the average travel times amongst each of the cars.\n",
    "    \n",
    "    Observation: \n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tObservation                 Min          Max\n",
    "        0\tPrevious Route Choice        0      total_routes-1\n",
    "        1\tRoute Travel Time            0           +Inf\n",
    "        2\tComm Message               -Inf          +Inf\n",
    "        \n",
    "    Actions:\n",
    "        ## FILL OUT WHEN DONE\n",
    "        Num\tAction                      Min          Max\n",
    "        0\tFuture Path_Choice           0      total_routes-1\n",
    "        1\tComm Message               -Inf          +Inf\n",
    "            \n",
    "    Reward:\n",
    "        Reward for each car is determined by the following formula: \n",
    "        marginal_cost = d[t(x_e)]/d[x_e]\n",
    "        Cost = route_travel_time + λ(marginal_cost)\n",
    "        Reward = -Cost\n",
    "        ***\n",
    "        route_travel_time: Travel time of the route previously taken by the car\n",
    "        marginal_cost: Cost that the car's route choice imposes on everyone else. \n",
    "                       The formula above captures the change in the travel flow \n",
    "                       with respect to the change in vehicle flow on a given road.\n",
    "        λ: Weight Toward Social Good (between 0 and 1)\n",
    "        \n",
    "    \n",
    "    Starting State:\n",
    "        All observations are assigned -1 for path choice and travel times.\n",
    "    \n",
    "    Episode Termination:\n",
    "        Cars keeps a consistent routing distribution.\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average travel time is less than or equal to the theorical social optimum. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        self.network_name = config['network']\n",
    "        self.num_paths = config['num_paths']\n",
    "        self.soc_fac = config['soc_fac']\n",
    "        self.num_veh = config['num_veh']\n",
    "        self.num_obs = 2\n",
    "        self.num_actions = 1\n",
    "        self.state = None\n",
    "        # Make observation space\n",
    "        #comm = []\n",
    "        obs_spaces = {\n",
    "            'prev_route': Discrete(self.num_paths),\n",
    "            'prev_time': Box(low=0, \n",
    "                             high=float('+inf'), \n",
    "                             shape=(1,), \n",
    "                             dtype=np.float32)\n",
    "            ,\n",
    "            'communication_part': Box(low=0, \n",
    "                             high=float('+inf'), \n",
    "                             shape=(4,), \n",
    "                             dtype=np.float32)\n",
    "        }\n",
    "        self.preprocessor = DictFlatteningPreprocessor(Dict(obs_spaces))\n",
    "        self.observation_space = self.preprocessor.observation_space\n",
    "        # Make the action space\n",
    "        self.action_space = Discrete(self.num_paths) # int between 0 and num_paths-1\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        return self.state\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        # Create initial observations for each vehicle\n",
    "        comm = []\n",
    "        start = {\n",
    "            # Add something here\n",
    "            'prev_route': 0,\n",
    "            'prev_time': 0,\n",
    "            'communication_path': comm\n",
    "        }\n",
    "        self.state = {'car_{}'.format(i): self.preprocessor.transform(start) for i in range(self.num_veh)}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        ### ADD THE COMMUNICATION CHANNEL\n",
    "        \"\"\"\n",
    "        FILL IN HERE.\n",
    "        \"\"\"\n",
    "        obs_dict, rew_dict, done, info_dict = {}, {}, {}, {}\n",
    "        \n",
    "        # Apply the actions of every agent at the same time\n",
    "        paths_flow_dict = {}\n",
    "        \n",
    "        for agent, rl_action in action_dict.items():\n",
    "            # agent is one string that represent the id of the agent\n",
    "            # rl_action is one number that represent the path choice of the agent,\n",
    "            # rl_action should be a int between 0 and nb_paths-1\n",
    "            rl_action = int(rl_action)\n",
    "            assert type(rl_action) == int and rl_action > -1 and rl_action < network.nb_paths\n",
    "            # we built a dictionnary paths_flow_dict that store the path flow on every path\n",
    "            if rl_action in paths_flow_dict:\n",
    "                paths_flow_dict[rl_action] += 1\n",
    "            else:\n",
    "                paths_flow_dict[rl_action] = 1\n",
    "\n",
    "        # update the path travel times of the network given the path flows\n",
    "        network.update_flow_from_dict(paths_flow_dict)\n",
    "        \n",
    "        communication_part = {}\n",
    "        for agent, path_choice in action_dict.items():\n",
    "            #communication_part[agent].append(path_choice)\n",
    "            travel_time[agent] = network.travel_time(path_choice)\n",
    "            communication_part[agent].append(travel_time[agent])\n",
    "        \n",
    "        \n",
    "        # Calculate states, reward, and done for each agent\n",
    "        travel_time = {}\n",
    "        marginal_cost = {}\n",
    "        \n",
    "        for agent, path_choice in action_dict.items():\n",
    "            path_choice = int(path_choice)\n",
    "            assert type(path_choice) == int and path_choice > -1 and path_choice < network.nb_paths\n",
    "            # network travel time ( path ) return the travel time of the path\n",
    "            travel_time[agent] = network.travel_time(path_choice)\n",
    "            # network marginal cost ( path ) return the marginal cost of the path\n",
    "            marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "            \n",
    "            new_obs = {\n",
    "                # Adding something here to change the observation space\n",
    "                'prev_route': path_choice,\n",
    "                'prev_time': travel_time[agent],\n",
    "                'communication': communication_part\n",
    "            }\n",
    "            obs_dict[agent] = self.preprocessor.transform(new_obs)\n",
    "            # Cost is the path_time\n",
    "            # rew_dict[agent] = reward_calculator(agent, marginal_cost)\n",
    "            # -path_choice # TO-DO: CHANGE THIS! \n",
    "            # Set done and infos\n",
    "            done[agent] = True\n",
    "            info_dict[agent] = {}\n",
    "        rew_dict = reward_calculator(travel_time, marginal_cost, self.soc_fac)\n",
    "        self.state = obs_dict   \n",
    "        \n",
    "        self.file = open(\"/Users/vikrantsingh/Projects/learning_wardrop/test_lambda_\" + str(self.soc_fac) + \"_gamma_\" + str(gamma) + \"_total_tt_2\", 'a')\n",
    "        self.file.write(\"Actions: \" + str(action_dict) + '\\n')\n",
    "        self.file.write(\"Reward: \" + str(rew_dict) + '\\n')\n",
    "        self.file.close()\n",
    "        \n",
    "        done[\"__all__\"] = True\n",
    "         \n",
    "        return obs_dict, rew_dict, done, info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code runs the experiment for the multiagent problem.\n",
    "\n",
    "Remark:\n",
    "On the Braess network using 4 vehicles, we should get:\n",
    "- if the social factor is 0, Nash: a reward of -3.75 in average, 2 cars on the first path, 1 on the second and third path\n",
    "- if the social factor is 1, Social optimum: a travel time of -3.5 in average (a reward of ), 2 cars on the first path, 1 on the second and third path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup policies for each vehicle\n",
    "\n",
    "# to change\n",
    "gamma = 0.5\n",
    "lambd = 0\n",
    "\n",
    "network_name = 'Braess'\n",
    "nb_veh = 4\n",
    "# network init should build the Network object\n",
    "# ----- TO DO -----: import the network class\n",
    "network = Networks.network(network_name, nb_veh)\n",
    "# nb_path should be a property method\n",
    "\n",
    "\"\"\"\n",
    "define a function (class instantiation) which have for parameter a network name, \n",
    "and the number of vehicles, \n",
    "the return the num paths.\n",
    "\n",
    "interface of the class network\n",
    "class network:\n",
    "    def __init__(self, network_name, nb_veh):\n",
    "        load the network which correspond to the network_name\n",
    "        define the nb_veh as the nb_veh\n",
    "        from nb_veh and the intern demand define the number of flow that each veh represent\n",
    "        also define __nb_paths to give it to the Env\n",
    "\n",
    "    @property\n",
    "    def nb_paths(self):\n",
    "        return self.__nb_paths\n",
    "\"\"\"\n",
    "\n",
    "nb_path = network.nb_paths\n",
    "\n",
    "env_config = {\n",
    "    'network': network_name,\n",
    "    'num_veh': nb_veh,\n",
    "    'num_paths': nb_path,\n",
    "    'soc_fac': lambd\n",
    "}\n",
    "routing_env = RoutingEnv(env_config)\n",
    "car_obs_space = routing_env.observation_space\n",
    "car_act_space = routing_env.action_space\n",
    "config = {\"gamma\": gamma}\n",
    "policy_graphs = {\n",
    "    'vehicles': (DQNPolicyGraph, car_obs_space, car_act_space, config)\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Begin\")\n",
    "    env_creator_name = 'multi_routing'\n",
    "    register_env(env_creator_name, lambda config: RoutingEnv(config))\n",
    "    ray.init()\n",
    "    experiments = {\n",
    "        'route-DQN': {\n",
    "            'run': 'DQN',\n",
    "            'env': 'multi_routing',\n",
    "            'stop': {\n",
    "                'training_iteration': 100\n",
    "            },\n",
    "            'config': {\n",
    "                'env_config': env_config,\n",
    "                'multiagent': {\n",
    "                    'policy_graphs': policy_graphs,\n",
    "                    'policy_mapping_fn': tune.function(lambda agent_id: 'vehicles')\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        # put additional experiments to run concurrently here\n",
    "    }\n",
    "    print(\"End\")\n",
    "    \n",
    "    run_experiments(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "### First, we use the travel time of the last iteration as the reward\n",
    "$\\gamma = 0$ and $\\lambda = 0$.\n",
    "We should obtain the nash equilibrium:\n",
    "Where the travel time are $[3.75, 3.75, 3.75]$\n",
    "\n",
    "### Then, we use marginal cost of the last iteration as the reward\n",
    "$\\gamma = 0$ and $\\lambda = 1$.\n",
    "We should obtain the social equilibrium:\n",
    "Where the travel time are $[3.25, 3.5, 3.5]$\n",
    "\n",
    "This is not the case here. It seems that the cars learn to do all the same thing.\n",
    "\n",
    "### We should try to add the bregman divergence in the reward (cf Walid, with a $\\gamma = 1$ (no regret online learning).\n",
    "### We should try to understand the learning process of the agents\n",
    "### We should also try to change the discount factor $\\gamma$\n",
    "### We should also add the communication process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tt_mc(action_dict, network, soc_fact):\n",
    "    paths_flow_dict = {}\n",
    "    for agent, rl_action in action_dict.items():\n",
    "        rl_action = int(rl_action)\n",
    "        if rl_action in paths_flow_dict:\n",
    "            paths_flow_dict[rl_action] += 1\n",
    "        else:\n",
    "            paths_flow_dict[rl_action] = 1\n",
    "    network.update_flow_from_dict(paths_flow_dict)\n",
    "\n",
    "    travel_time = {}\n",
    "    marginal_cost = {}\n",
    "    for agent, path_choice in action_dict.items():\n",
    "        path_choice = int(path_choice)\n",
    "        travel_time[agent] = network.travel_time(path_choice)\n",
    "        marginal_cost[agent] = network.marginal_cost(path_choice)\n",
    "    rew_dict = reward_calculator(travel_time, marginal_cost, soc_fact)\n",
    "    return travel_time, marginal_cost, rew_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "file = open(\"/Users/theophile/Documents/Classes/FLOW/Project/learning_wardrop/test_lambda_0_gamma_0.5_total_tt_2\", 'r')\n",
    "j = 0\n",
    "# we want to plot the evolution of the path choice, of the reward and of the travel time\n",
    "Actions_plot = np.array([[0, 0, 0, 0]])\n",
    "Reward_plot = np.array([[0, 0, 0, 0]])\n",
    "Travel_time_plot = np.array([[0, 0, 0, 0]])\n",
    "while(True):\n",
    "    j = j+1\n",
    "    try:\n",
    "        actions = file.readline()\n",
    "        rewards = file.readline()\n",
    "        action_dict = ast.literal_eval(\"{\" + actions.split('{')[1].split('}')[0]+ \"}\")\n",
    "        reward_dict = ast.literal_eval(\"{\" + rewards.split('{')[1].split('}')[0]+ \"}\")\n",
    "    \n",
    "        network = Networks.network(network_name, nb_veh)\n",
    "        travel_time, marginal_cost, rew_dict = get_tt_mc(action_dict, network, 0)\n",
    "        \n",
    "        actions_np = np.fromiter(action_dict.values(), dtype=int)\n",
    "        Actions_plot = np.append(Actions_plot, [actions_np], axis=0)\n",
    "        rewards_np = np.fromiter(reward_dict.values(), dtype=float)\n",
    "        Reward_plot = np.append(Reward_plot, [rewards_np], axis=0)\n",
    "        travel_time_np = np.fromiter(travel_time.values(), dtype=float)\n",
    "        Travel_time_plot = np.append(Travel_time_plot, [travel_time_np], axis=0)\n",
    "        if(j==1):\n",
    "            print(\"------ First iteration ------\")\n",
    "            print(\"Path choice: \" + str(action_dict))\n",
    "            print(\"Reward ray: \" + str(reward_dict))\n",
    "            print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "            print(\"Travel time cars: \" + str(travel_time))\n",
    "            print(\"Marginal cost: \" + str(marginal_cost))\n",
    "            print(\"Reward network: \" + str(rew_dict))\n",
    "    except:\n",
    "        print()\n",
    "        print(\"------ Last iteration ------\")\n",
    "        print(\"Path choice: \" + str(action_dict))\n",
    "        print(\"Reward ray: \" + str(reward_dict))\n",
    "        print(\"Travel time paths: \" + str({\"path \" + str(i): network.travel_time(i) for i in range(3)}))\n",
    "        print(\"Travel time cars: \" + str(travel_time))\n",
    "        print(\"Marginal cost: \" + str(marginal_cost))\n",
    "        print(\"Reward network: \" + str(rew_dict))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(4):\n",
    "    plt.plot(Actions_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Path choice of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(Reward_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Rewards of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(Travel_time_plot[1:,i], \"+\")\n",
    "    plt.ylabel(\"Travel time of car \" + str(i))\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_part=Box(low=0, high=float('+inf'),shape=(3,),dtype=np.float32)\n",
    "communication_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for agent, path_choice in action_dict.items():\n",
    "communication_part['a']=1.0\n",
    "            #travel_time[agent] = network.travel_time(path_choice)\n",
    "            #communication_part.append(travel_time[agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Environment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
